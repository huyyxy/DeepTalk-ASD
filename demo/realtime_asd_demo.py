#!/usr/bin/env python3
"""
realtime_asd_demo.py

å®æ—¶æ´»åŠ¨è¯´è¯è€…æ£€æµ‹ (Active Speaker Detection) æ¼”ç¤ºç¨‹åºã€‚
ä»æ‘„åƒå¤´é‡‡é›†è§†é¢‘å¸§ã€éº¦å…‹é£é‡‡é›†éŸ³é¢‘å¸§ï¼Œé€šè¿‡ DeepTalk-ASD åˆ¤æ–­è§†é¢‘ç”»é¢ä¸­
å“ªä¸ªäººè„¸åœ¨è¯´è¯ï¼Œå¹¶ç”¨ OpenCV å®æ—¶æ˜¾ç¤ºæ£€æµ‹ç»“æœã€‚

ç”¨æ³•:
    python3 demo/realtime_asd_demo.py [OPTIONS]

    # ä½¿ç”¨ç¼ºçœæ‘„åƒå¤´å’Œéº¦å…‹é£:
    python3 demo/realtime_asd_demo.py

    # æŒ‡å®šæ‘„åƒå¤´å’Œéº¦å…‹é£:
    python3 demo/realtime_asd_demo.py --camera 1 --microphone 2

    # è‡ªå®šä¹‰ ASD ç»„ä»¶:
    python3 demo/realtime_asd_demo.py --face-detector inspireface --speaker-detector LR-ASD-ONNX
"""

import os
import sys
import time
import argparse
import threading
import struct
import traceback

import cv2
import numpy as np

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

from deeptalk_asd import ASDDetectorFactory, VideoFrame, VideoBufferType, AudioFrame, TurnState


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸é‡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AUDIO_CHUNK_MS = 30          # æ¯æ¬¡é‡‡é›†çš„éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
SPEAKING_PERSIST_SEC = 0.5   # è¯´è¯è€…ç»¿æ¡†æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
FONT = cv2.FONT_HERSHEY_SIMPLEX
FONT_SCALE = 0.5
FONT_THICKNESS = 1
BOX_THICKNESS = 2
COLOR_SPEAKING = (0, 255, 0)     # ç»¿è‰² (BGR)
COLOR_NOT_SPEAKING = (0, 0, 255) # çº¢è‰² (BGR)
COLOR_TEXT_BG = (0, 0, 0)        # æ–‡å­—èƒŒæ™¯è‰²
COLOR_TEXT_FG = (255, 255, 255)  # æ–‡å­—å‰æ™¯è‰²


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="DeepTalk-ASD å®æ—¶æ´»åŠ¨è¯´è¯è€…æ£€æµ‹æ¼”ç¤º"
    )

    # è®¾å¤‡å‚æ•°
    parser.add_argument(
        "--camera", type=int, default=0,
        help="æ‘„åƒå¤´è®¾å¤‡ç´¢å¼• (é»˜è®¤: 0)"
    )
    parser.add_argument(
        "--camera-width", type=int, default=640,
        help="æ‘„åƒå¤´é‡‡é›†å®½åº¦ (é»˜è®¤: 640)"
    )
    parser.add_argument(
        "--camera-height", type=int, default=480,
        help="æ‘„åƒå¤´é‡‡é›†é«˜åº¦ (é»˜è®¤: 480)"
    )
    parser.add_argument(
        "--microphone", type=int, default=None,
        help="éº¦å…‹é£è®¾å¤‡ç´¢å¼• (é»˜è®¤: ç³»ç»Ÿç¼ºçœè®¾å¤‡)"
    )
    parser.add_argument(
        "--sample-rate", type=int, default=16000,
        help="éŸ³é¢‘é‡‡æ ·ç‡ (é»˜è®¤: 16000)"
    )

    # ASD ç»„ä»¶å‚æ•°
    parser.add_argument(
        "--face-detector", type=str, default="inspireface",
        help="äººè„¸æ£€æµ‹å™¨ç±»å‹ (é»˜è®¤: inspireface)"
    )
    parser.add_argument(
        "--turn-detector", type=str, default="silero-vad",
        help="è½®æ¬¡æ£€æµ‹å™¨ç±»å‹ (é»˜è®¤: silero-vad)"
    )
    parser.add_argument(
        "--speaker-detector", type=str, default="LR-ASD-ONNX",
        help="è¯´è¯è€…æ£€æµ‹å™¨ç±»å‹ (é»˜è®¤: LR-ASD-ONNX)"
    )
    parser.add_argument(
        "--onnx-dir", type=str, default=None,
        help="ONNX æƒé‡ç›®å½• (é»˜è®¤: <project>/weights)"
    )
    parser.add_argument(
        "--vad-model-path", type=str, default=None,
        help="VAD æ¨¡å‹æ–‡ä»¶è·¯å¾„ (å¯é€‰)"
    )

    return parser.parse_args()


class SpeakingStateTracker:
    """
    çº¿ç¨‹å®‰å…¨çš„è¯´è¯çŠ¶æ€ç®¡ç†å™¨ã€‚

    è®°å½•æ¯ä¸ª track_id æœ€åä¸€æ¬¡è¢«åˆ¤å®šä¸ºè¯´è¯è€…çš„æ—¶é—´æˆ³ï¼Œ
    å¹¶æä¾›æŸ¥è¯¢æ¥å£åˆ¤æ–­è¯¥ track_id æ˜¯å¦ä»å¤„äº"è¯´è¯"çŠ¶æ€ï¼ˆ0.5 ç§’å†…ï¼‰ã€‚
    """

    def __init__(self, persist_sec: float = SPEAKING_PERSIST_SEC):
        self._speaking_faces: dict[int, float] = {}  # track_id -> timestamp
        self._lock = threading.Lock()
        self._persist_sec = persist_sec

    def update_speakers(self, speaker_scores: dict):
        """
        æ›´æ–°è¯´è¯è€…çŠ¶æ€ã€‚

        å‚æ•°:
            speaker_scores: evaluate() è¿”å›çš„ {track_id: score, ...}
        """
        if not speaker_scores:
            return
        now = time.perf_counter()
        with self._lock:
            for track_id, score in speaker_scores.items():
                if score > 0:
                    self._speaking_faces[track_id] = now

    def is_speaking(self, track_id: int) -> bool:
        """åˆ¤æ–­æŸä¸ª track_id æ˜¯å¦ä»åœ¨"è¯´è¯"çŠ¶æ€"""
        now = time.perf_counter()
        with self._lock:
            last_time = self._speaking_faces.get(track_id)
            if last_time is None:
                return False
            if now - last_time > self._persist_sec:
                del self._speaking_faces[track_id]
                return False
            return True

    def cleanup(self):
        """æ¸…ç†è¿‡æœŸçš„è¯´è¯çŠ¶æ€"""
        now = time.perf_counter()
        with self._lock:
            expired = [
                tid for tid, ts in self._speaking_faces.items()
                if now - ts > self._persist_sec
            ]
            for tid in expired:
                del self._speaking_faces[tid]


class AudioCaptureThread(threading.Thread):
    """
    éŸ³é¢‘é‡‡é›†çº¿ç¨‹ã€‚

    ä½¿ç”¨ PyAudio ä»éº¦å…‹é£é‡‡é›†éŸ³é¢‘ï¼Œè°ƒç”¨ ASD çš„ append_audio / evaluate æ–¹æ³•ï¼Œ
    å¹¶å°†è¯´è¯è€…ç»“æœæ›´æ–°åˆ° SpeakingStateTrackerã€‚
    """

    def __init__(self, asd, state_tracker: SpeakingStateTracker,
                 sample_rate: int = 16000, device_index: int = None):
        super().__init__(daemon=True)
        self._asd = asd
        self._state_tracker = state_tracker
        self._sample_rate = sample_rate
        self._device_index = device_index
        self._running = False
        self._chunk_samples = int(sample_rate * AUDIO_CHUNK_MS / 1000)  # æ¯ chunk é‡‡æ ·æ•°

    def run(self):
        """éŸ³é¢‘é‡‡é›†ä¸»å¾ªç¯"""
        import pyaudio

        pa = pyaudio.PyAudio()
        self._running = True

        # æ‰“å°å¯ç”¨éº¦å…‹é£è®¾å¤‡
        print("\n--- å¯ç”¨éŸ³é¢‘è¾“å…¥è®¾å¤‡ ---")
        for i in range(pa.get_device_count()):
            dev_info = pa.get_device_info_by_index(i)
            if dev_info["maxInputChannels"] > 0:
                marker = " <-- å½“å‰" if self._device_index is not None and i == self._device_index else ""
                if self._device_index is None and i == pa.get_default_input_device_info()['index']:
                    marker = " <-- ç¼ºçœ"
                print(f"  [{i}] {dev_info['name']} (è¾“å…¥é€šé“: {dev_info['maxInputChannels']}){marker}")
        print("---\n")

        stream_kwargs = {
            "format": pyaudio.paInt16,
            "channels": 1,
            "rate": self._sample_rate,
            "input": True,
            "frames_per_buffer": self._chunk_samples,
        }
        if self._device_index is not None:
            stream_kwargs["input_device_index"] = self._device_index

        try:
            stream = pa.open(**stream_kwargs)
        except Exception as e:
            print(f"[é”™è¯¯] æ— æ³•æ‰“å¼€éº¦å…‹é£: {e}")
            traceback.print_exc()
            pa.terminate()
            return

        print(f"[éŸ³é¢‘] éº¦å…‹é£å·²å¼€å¯ (é‡‡æ ·ç‡: {self._sample_rate}, chunk: {self._chunk_samples} samples)")

        try:
            while self._running:
                try:
                    raw_data = stream.read(self._chunk_samples, exception_on_overflow=False)
                except Exception as e:
                    print(f"[è­¦å‘Š] éŸ³é¢‘è¯»å–å¼‚å¸¸: {e}")
                    continue

                create_time = time.perf_counter()

                # å°è£…ä¸º AudioFrame
                audio_frame = AudioFrame(
                    data=raw_data,
                    sample_rate=self._sample_rate,
                    num_channels=1,
                    samples_per_channel=self._chunk_samples,
                )

                # è°ƒç”¨ ASD append_audio è¿›è¡Œ VAD æ£€æµ‹
                utterance = self._asd.append_audio(audio_frame, create_time)

                if utterance is not None and utterance.turn_state == TurnState.TURN_END:
                    # VAD æ£€æµ‹åˆ°è¯´è¯ç»“æŸï¼Œè¯„ä¼°æ´»åŠ¨è¯´è¯è€…
                    speaker_scores = self._asd.evaluate()
                    if speaker_scores:
                        self._state_tracker.update_speakers(speaker_scores)
                        print(f"[ASD] è¯´è¯è€…è¯„ä¼°ç»“æœ: {speaker_scores}")

        except Exception as e:
            print(f"[é”™è¯¯] éŸ³é¢‘çº¿ç¨‹å¼‚å¸¸: {e}")
            traceback.print_exc()
        finally:
            stream.stop_stream()
            stream.close()
            pa.terminate()
            print("[éŸ³é¢‘] éº¦å…‹é£å·²å…³é—­")

    def stop(self):
        """åœæ­¢éŸ³é¢‘é‡‡é›†"""
        self._running = False


def draw_face_overlay(frame: np.ndarray, face_profile, is_speaking: bool):
    """
    åœ¨è§†é¢‘å¸§ä¸Šç»˜åˆ¶äººè„¸æ¡†å’Œä¿¡æ¯æ ‡æ³¨ã€‚

    å‚æ•°:
        frame: OpenCV BGR å›¾åƒ (numpy.ndarray)
        face_profile: FaceProfile å®ä¾‹
        is_speaking: è¯¥äººè„¸æ˜¯å¦æ­£åœ¨è¯´è¯
    """
    rect = face_profile.face_rectangle
    x = int(rect.x)
    y = int(rect.y)
    w = int(rect.width)
    h = int(rect.height)

    # é€‰æ‹©è¾¹æ¡†é¢œè‰²
    color = COLOR_SPEAKING if is_speaking else COLOR_NOT_SPEAKING

    # ç»˜åˆ¶äººè„¸è¾¹æ¡†
    cv2.rectangle(frame, (x, y), (x + w, y + h), color, BOX_THICKNESS)

    # æ„å»ºä¿¡æ¯æ–‡æœ¬
    info_parts = [f"ID:{face_profile.track_id}"]
    if face_profile.gender:
        info_parts.append(face_profile.gender)
    if face_profile.age:
        info_parts.append(face_profile.age)
    if face_profile.emotion:
        info_parts.append(face_profile.emotion)
    if is_speaking:
        info_parts.append("ğŸ”Š")

    info_text = " | ".join(info_parts)

    # è®¡ç®—æ–‡å­—å¤§å°å’Œä½ç½®
    (text_w, text_h), baseline = cv2.getTextSize(info_text, FONT, FONT_SCALE, FONT_THICKNESS)
    text_x = x
    text_y = y - 8  # æ–‡å­—åœ¨è¾¹æ¡†ä¸Šæ–¹

    # ç»˜åˆ¶æ–‡å­—èƒŒæ™¯
    cv2.rectangle(
        frame,
        (text_x, text_y - text_h - baseline),
        (text_x + text_w, text_y + baseline),
        COLOR_TEXT_BG, -1
    )

    # ç»˜åˆ¶æ–‡å­—
    cv2.putText(
        frame, info_text,
        (text_x, text_y),
        FONT, FONT_SCALE, COLOR_TEXT_FG, FONT_THICKNESS, cv2.LINE_AA
    )


def create_asd(args):
    """
    æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åˆ›å»º ASD å®ä¾‹ã€‚

    å‚æ•°:
        args: argparse å‘½åç©ºé—´

    è¿”å›:
        ASDInterface å®ä¾‹
    """
    # ç¡®å®š ONNX ç›®å½•
    onnx_dir = args.onnx_dir
    if onnx_dir is None:
        onnx_dir = os.path.join(PROJECT_ROOT, "weights")

    # äººè„¸æ£€æµ‹å™¨é…ç½®
    face_detector_config = {
        "type": args.face_detector,
    }

    # è½®æ¬¡æ£€æµ‹å™¨é…ç½®
    vad_model_path = args.vad_model_path
    if vad_model_path is None:
        vad_model_path = os.path.join(PROJECT_ROOT, "weights", "silero_vad.onnx")
    turn_detector_config = {
        "type": args.turn_detector,
        "model_path": vad_model_path,
    }

    # è¯´è¯è€…æ£€æµ‹å™¨é…ç½®
    speaker_detector_config = {
        "type": args.speaker_detector,
        "onnx_dir": onnx_dir,
    }

    print(f"[ASD] åˆ›å»º ASD å®ä¾‹...")
    print(f"  äººè„¸æ£€æµ‹å™¨: {face_detector_config}")
    print(f"  è½®æ¬¡æ£€æµ‹å™¨: {turn_detector_config}")
    print(f"  è¯´è¯è€…æ£€æµ‹å™¨: {speaker_detector_config}")

    factory = ASDDetectorFactory(
        face_detector=face_detector_config,
        turn_detector=turn_detector_config,
        speaker_detector=speaker_detector_config,
    )
    asd = factory.create()
    if asd is None:
        raise RuntimeError("ASD å®ä¾‹åˆ›å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç»„ä»¶é…ç½®å’Œæ¨¡å‹æ–‡ä»¶")
    print("[ASD] åˆ›å»ºæˆåŠŸ")
    return asd


def main():
    args = parse_args()

    # 1. åˆ›å»º ASD å®ä¾‹
    asd = create_asd(args)

    # 2. åˆ›å»ºè¯´è¯çŠ¶æ€è·Ÿè¸ªå™¨
    state_tracker = SpeakingStateTracker()

    # 3. å¯åŠ¨éŸ³é¢‘é‡‡é›†çº¿ç¨‹
    audio_thread = AudioCaptureThread(
        asd=asd,
        state_tracker=state_tracker,
        sample_rate=args.sample_rate,
        device_index=args.microphone,
    )
    audio_thread.start()

    # 4. æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(args.camera)
    if not cap.isOpened():
        print(f"[é”™è¯¯] æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {args.camera}")
        audio_thread.stop()
        return

    cap.set(cv2.CAP_PROP_FRAME_WIDTH, args.camera_width)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, args.camera_height)

    # ç­‰å¾…æ‘„åƒå¤´é¢„çƒ­ï¼ˆæŸäº›è™šæ‹Ÿæ‘„åƒå¤´æ’ä»¶éœ€è¦åˆå§‹åŒ–æ—¶é—´ï¼‰
    print("[è§†é¢‘] ç­‰å¾…æ‘„åƒå¤´é¢„çƒ­...")
    time.sleep(2.0)

    actual_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    print(f"[è§†é¢‘] æ‘„åƒå¤´å·²å¼€å¯: è®¾å¤‡ {args.camera}, åˆ†è¾¨ç‡ {actual_w}x{actual_h}")
    print("[æç¤º] æŒ‰ 'q' é”®é€€å‡º\n")

    window_name = "DeepTalk-ASD å®æ—¶æ¼”ç¤º"
    cv2.namedWindow(window_name, cv2.WINDOW_AUTOSIZE)

    read_fail_count = 0
    MAX_READ_FAILURES = 30  # æœ€å¤§è¿ç»­è¯»å–å¤±è´¥æ¬¡æ•°

    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                read_fail_count += 1
                if read_fail_count >= MAX_READ_FAILURES:
                    print(f"[é”™è¯¯] è¿ç»­ {MAX_READ_FAILURES} æ¬¡æ— æ³•è¯»å–æ‘„åƒå¤´å¸§ï¼Œé€€å‡º")
                    break
                print(f"[è­¦å‘Š] æ— æ³•è¯»å–æ‘„åƒå¤´å¸§ (é‡è¯• {read_fail_count}/{MAX_READ_FAILURES})")
                time.sleep(0.1)
                continue
            read_fail_count = 0  # è¯»å–æˆåŠŸï¼Œé‡ç½®è®¡æ•°å™¨

            create_time = time.perf_counter()

            # å°† OpenCV BGR å¸§è½¬æ¢ä¸º RGB24 æ ¼å¼çš„ VideoFrame
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w, _ = frame_rgb.shape
            video_frame = VideoFrame(
                width=w,
                height=h,
                type=VideoBufferType.RGB24,
                data=bytes(frame_rgb),
            )

            # è°ƒç”¨ ASD å¤„ç†è§†é¢‘å¸§
            face_profiles = asd.append_video(video_frame, create_time)

            # æ¸…ç†è¿‡æœŸçš„è¯´è¯çŠ¶æ€
            state_tracker.cleanup()

            # ç»˜åˆ¶äººè„¸æ¡†å’Œä¿¡æ¯
            if face_profiles:
                for profile in face_profiles:
                    is_speaking = state_tracker.is_speaking(profile.track_id)
                    draw_face_overlay(frame, profile, is_speaking)

            # æ˜¾ç¤ºå¸§
            cv2.imshow(window_name, frame)

            # æ£€æŸ¥é€€å‡ºé”®
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q') or key == 27:  # q æˆ– ESC
                break

            # æ£€æŸ¥çª—å£æ˜¯å¦å·²å…³é—­
            if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
                break

    except KeyboardInterrupt:
        print("\n[ä¿¡æ¯] æ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
    finally:
        # æ¸…ç†èµ„æº
        audio_thread.stop()
        audio_thread.join(timeout=2.0)
        cap.release()
        cv2.destroyAllWindows()
        print("[ä¿¡æ¯] ç¨‹åºå·²é€€å‡º")


if __name__ == "__main__":
    main()
